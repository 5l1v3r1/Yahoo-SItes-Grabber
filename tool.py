import requestsfrom bs4 import BeautifulSoupfrom urllib.parse import urlparsefrom multiprocessing.dummy import Pool as ThreadPoolsites = open('ip.txt', 'r').read().split('\n')print(""" 		[#] Script ::     _____           _          _   _             _   _  _____         /  __ \         | |        | | | |           | | | ||  ___|        | /  \/ ___   __| | ___  __| | | |__  _   _  | |_| || |____  __    | |    / _ \ / _` |/ _ \/ _` | | '_ \| | | | |  _  ||  __\ \/ /    | \__/\ (_) | (_| |  __/ (_| | | |_) | |_| | | | | || |___>  <     \_____/\___/ \__,_|\___|\__,_| |_.__/ \__, | \_| |_/\____/_/\_\\                                           __/ |                                 #Yahoo Sites Grabber ::     |___/          """)def BingBot(url):	try:		headers = {'user-agent': 'Googlebot/2.1 (+http://www.google.com/bot.html)'}		req = requests.get('https://search.yahoo.com/search?q=ip:'+url+'&first=1', headers=headers)		r1 = req.text		soup = BeautifulSoup(r1, 'html.parser')		re = soup.find('ol', {'class': 'mb-15 reg searchCenterMiddle'})		l = str(re.findAll('span', {'class': 'fz-ms fw-m fc-12th wr-bw lh-17'}))		for item in l:			try:				a = (l.replace('<span class="fz-ms fw-m fc-12th wr-bw lh-17">', 'http://'))				b = (a.replace('</span>', '/'))				c = (b.replace('<b>', ''))				f = (c.replace('</b>', ''))				g = (f.replace(' ', ''))				h = (g.replace('[', ''))				i = (h.replace(']', ''))				words = str(i).split(",")				for i in words:					open('links.txt', 'a+').close()					data = urlparse(i).hostname					if data not in open('links.txt', 'r').read():						open('links.txt', 'a+').write(data + '\n')						print('[#] ' + data)			except:				pass		if 'b=11' in r1:			req = requests.get('https://search.yahoo.com/search?q=ip:'+url+'&first=11&b=11', headers=headers)			r2 = req.text			soup = BeautifulSoup(r2, 'html.parser')			re = soup.find('ol', {'class': 'mb-15 reg searchCenterMiddle'})			l = str(re.findAll('span', {'class': 'fz-ms fw-m fc-12th wr-bw lh-17'}))			for item in l:				try:					a = (l.replace('<span class="fz-ms fw-m fc-12th wr-bw lh-17">', 'http://'))					b = (a.replace('</span>', '/'))					c = (b.replace('<b>', ''))					f = (c.replace('</b>', ''))					g = (f.replace(' ', ''))					h = (g.replace('[', ''))					i = (h.replace(']', ''))					words = str(i).split(",")					for i in words:						open('links.txt', 'a+').close()						data = urlparse(i).hostname						if data not in open('links.txt', 'r').read():							open('links.txt', 'a+').write(data + '\n')							print('[#] ' + data)				except:					pass		if 'b=21' in r2:			req = requests.get('https://search.yahoo.com/search?q=ip:'+url+'&first=21&b=21', headers=headers)			r3 = req.text			soup = BeautifulSoup(r3, 'html.parser')			re = soup.find('ol', {'class': 'mb-15 reg searchCenterMiddle'})			l = str(re.findAll('span', {'class': 'fz-ms fw-m fc-12th wr-bw lh-17'}))			for item in l:				try:					a = (l.replace('<span class="fz-ms fw-m fc-12th wr-bw lh-17">', 'http://'))					b = (a.replace('</span>', '/'))					c = (b.replace('<b>', ''))					f = (c.replace('</b>', ''))					g = (f.replace(' ', ''))					h = (g.replace('[', ''))					i = (h.replace(']', ''))					words = str(i).split(",")					for i in words:						open('links.txt', 'a+').close()						data = urlparse(i).hostname						if data not in open('links.txt', 'r').read():							open('links.txt', 'a+').write(data + '\n')							print('[#] ' + data)				except:					pass		if 'b=31' in r3:			req = requests.get('https://search.yahoo.com/search?q=ip:'+url+'&first=31&b=31', headers=headers)			r4 = req.text			soup = BeautifulSoup(r4, 'html.parser')			re = soup.find('ol', {'class': 'mb-15 reg searchCenterMiddle'})			l = str(re.findAll('span', {'class': 'fz-ms fw-m fc-12th wr-bw lh-17'}))			for item in l:				try:					a = (l.replace('<span class="fz-ms fw-m fc-12th wr-bw lh-17">', 'http://'))					b = (a.replace('</span>', '/'))					c = (b.replace('<b>', ''))					f = (c.replace('</b>', ''))					g = (f.replace(' ', ''))					h = (g.replace('[', ''))					i = (h.replace(']', ''))					words = str(i).split(",")					for i in words:						open('links.txt', 'a+').close()						data = urlparse(i).hostname						if data not in open('links.txt', 'r').read():							open('links.txt', 'a+').write(data + '\n')							print('[#] ' + data)				except:					pass		if 'b=41' in r4:			req = requests.get('https://search.yahoo.com/search?q=ip:'+url+'&first=41&b=41', headers=headers)			r5 = req.text			soup = BeautifulSoup(r5, 'html.parser')			re = soup.find('ol', {'class': 'mb-15 reg searchCenterMiddle'})			l = str(re.findAll('span', {'class': 'fz-ms fw-m fc-12th wr-bw lh-17'}))			for item in l:				try:					a = (l.replace('<span class="fz-ms fw-m fc-12th wr-bw lh-17">', 'http://'))					b = (a.replace('</span>', '/'))					c = (b.replace('<b>', ''))					f = (c.replace('</b>', ''))					g = (f.replace(' ', ''))					h = (g.replace('[', ''))					i = (h.replace(']', ''))					words = str(i).split(",")					for i in words:						open('links.txt', 'a+').close()						data = urlparse(i).hostname						if data not in open('links.txt', 'r').read():							open('links.txt', 'a+').write(data + '\n')							print('[#] ' + data)				except:					pass		if 'b=51' in r5:			req = requests.get('https://search.yahoo.com/search?q=ip:'+url+'&first=51&b=51', headers=headers)			r6 = req.text			soup = BeautifulSoup(r6, 'html.parser')			re = soup.find('ol', {'class': 'mb-15 reg searchCenterMiddle'})			l = str(re.findAll('span', {'class': 'fz-ms fw-m fc-12th wr-bw lh-17'}))			for item in l:				try:					a = (l.replace('<span class="fz-ms fw-m fc-12th wr-bw lh-17">', 'http://'))					b = (a.replace('</span>', '/'))					c = (b.replace('<b>', ''))					f = (c.replace('</b>', ''))					g = (f.replace(' ', ''))					h = (g.replace('[', ''))					i = (h.replace(']', ''))					words = str(i).split(",")					for i in words:						open('links.txt', 'a+').close()						data = urlparse(i).hostname						if data not in open('links.txt', 'r').read():							open('links.txt', 'a+').write(data + '\n')							print('[#] ' + data)				except:					pass		if 'b=61' in r6:			req = requests.get('https://search.yahoo.com/search?q=ip:'+url+'&first=61&b=61', headers=headers)			r7 = req.text			soup = BeautifulSoup(r7, 'html.parser')			re = soup.find('ol', {'class': 'mb-15 reg searchCenterMiddle'})			l = str(re.findAll('span', {'class': 'fz-ms fw-m fc-12th wr-bw lh-17'}))			for item in l:				try:					a = (l.replace('<span class="fz-ms fw-m fc-12th wr-bw lh-17">', 'http://'))					b = (a.replace('</span>', '/'))					c = (b.replace('<b>', ''))					f = (c.replace('</b>', ''))					g = (f.replace(' ', ''))					h = (g.replace('[', ''))					i = (h.replace(']', ''))					words = str(i).split(",")					for i in words:						open('links.txt', 'a+').close()						data = urlparse(i).hostname						if data not in open('links.txt', 'r').read():							open('links.txt', 'a+').write(data + '\n')							print('[#] ' + data)				except:					pass		if 'b=71' in r7:			req = requests.get('https://search.yahoo.com/search?q=ip:'+url+'&first=71&b=71', headers=headers)			r8 = req.text			soup = BeautifulSoup(r8, 'html.parser')			re = soup.find('ol', {'class': 'mb-15 reg searchCenterMiddle'})			l = str(re.findAll('span', {'class': 'fz-ms fw-m fc-12th wr-bw lh-17'}))			for item in l:				try:					a = (l.replace('<span class="fz-ms fw-m fc-12th wr-bw lh-17">', 'http://'))					b = (a.replace('</span>', '/'))					c = (b.replace('<b>', ''))					f = (c.replace('</b>', ''))					g = (f.replace(' ', ''))					h = (g.replace('[', ''))					i = (h.replace(']', ''))					words = str(i).split(",")					for i in words:						open('links.txt', 'a+').close()						data = urlparse(i).hostname						if data not in open('links.txt', 'r').read():							open('links.txt', 'a+').write(data + '\n')							print('[#] ' + data)				except:					pass		if 'b=81' in r8:			req = requests.get('https://search.yahoo.com/search?q=ip:'+url+'&first=81&b=81', headers=headers)			r9 = req.text			soup = BeautifulSoup(r9, 'html.parser')			re = soup.find('ol', {'class': 'mb-15 reg searchCenterMiddle'})			l = str(re.findAll('span', {'class': 'fz-ms fw-m fc-12th wr-bw lh-17'}))			for item in l:				try:					a = (l.replace('<span class="fz-ms fw-m fc-12th wr-bw lh-17">', 'http://'))					b = (a.replace('</span>', '/'))					c = (b.replace('<b>', ''))					f = (c.replace('</b>', ''))					g = (f.replace(' ', ''))					h = (g.replace('[', ''))					i = (h.replace(']', ''))					words = str(i).split(",")					for i in words:						open('links.txt', 'a+').close()						data = urlparse(i).hostname						if data not in open('links.txt', 'r').read():							open('links.txt', 'a+').write(data + '\n')							print('[#] ' + data)				except:					pass		if 'b=91' in r9:			req = requests.get('https://search.yahoo.com/search?q=ip:'+url+'&first=91&b=91', headers=headers)			r10 = req.text			soup = BeautifulSoup(r10, 'html.parser')			re = soup.find('ol', {'class': 'mb-15 reg searchCenterMiddle'})			l = str(re.findAll('span', {'class': 'fz-ms fw-m fc-12th wr-bw lh-17'}))			for item in l:				try:					a = (l.replace('<span class="fz-ms fw-m fc-12th wr-bw lh-17">', 'http://'))					b = (a.replace('</span>', '/'))					c = (b.replace('<b>', ''))					f = (c.replace('</b>', ''))					g = (f.replace(' ', ''))					h = (g.replace('[', ''))					i = (h.replace(']', ''))					words = str(i).split(",")					for i in words:						open('links.txt', 'a+').close()						data = urlparse(i).hostname						if data not in open('links.txt', 'r').read():							open('links.txt', 'a+').write(data + '\n')							print('[#] ' + data)				except:					pass		if 'b=101' in r10:			req = requests.get('https://search.yahoo.com/search?q=ip:'+url+'&first=101&b=101', headers=headers)			r11 = req.text			soup = BeautifulSoup(r11, 'html.parser')			re = soup.find('ol', {'class': 'mb-15 reg searchCenterMiddle'})			l = str(re.findAll('span', {'class': 'fz-ms fw-m fc-12th wr-bw lh-17'}))			for item in l:				try:					a = (l.replace('<span class="fz-ms fw-m fc-12th wr-bw lh-17">', 'http://'))					b = (a.replace('</span>', '/'))					c = (b.replace('<b>', ''))					f = (c.replace('</b>', ''))					g = (f.replace(' ', ''))					h = (g.replace('[', ''))					i = (h.replace(']', ''))					words = str(i).split(",")					for i in words:						open('links.txt', 'a+').close()						data = urlparse(i).hostname						if data not in open('links.txt', 'r').read():							open('links.txt', 'a+').write(data + '\n')							print('[#] ' + data)				except:					pass		if 'b=111' in r11:			req = requests.get('https://search.yahoo.com/search?q=ip:'+url+'&first=111&b=111', headers=headers)			r12 = req.text			soup = BeautifulSoup(r12, 'html.parser')			re = soup.find('ol', {'class': 'mb-15 reg searchCenterMiddle'})			l = str(re.findAll('span', {'class': 'fz-ms fw-m fc-12th wr-bw lh-17'}))			for item in l:				try:					a = (l.replace('<span class="fz-ms fw-m fc-12th wr-bw lh-17">', 'http://'))					b = (a.replace('</span>', '/'))					c = (b.replace('<b>', ''))					f = (c.replace('</b>', ''))					g = (f.replace(' ', ''))					h = (g.replace('[', ''))					i = (h.replace(']', ''))					words = str(i).split(",")					for i in words:						open('links.txt', 'a+').close()						data = urlparse(i).hostname						if data not in open('links.txt', 'r').read():							open('links.txt', 'a+').write(data + '\n')							print('[#] ' + data)				except:					pass		if 'b=121' in r12:			req = requests.get('https://search.yahoo.com/search?q=ip:'+url+'&first=121&b=121', headers=headers)			r13 = req.text			soup = BeautifulSoup(r13, 'html.parser')			re = soup.find('ol', {'class': 'mb-15 reg searchCenterMiddle'})			l = str(re.findAll('span', {'class': 'fz-ms fw-m fc-12th wr-bw lh-17'}))			for item in l:				try:					a = (l.replace('<span class="fz-ms fw-m fc-12th wr-bw lh-17">', 'http://'))					b = (a.replace('</span>', '/'))					c = (b.replace('<b>', ''))					f = (c.replace('</b>', ''))					g = (f.replace(' ', ''))					h = (g.replace('[', ''))					i = (h.replace(']', ''))					words = str(i).split(",")					for i in words:						open('links.txt', 'a+').close()						data = urlparse(i).hostname						if data not in open('links.txt', 'r').read():							open('links.txt', 'a+').write(data + '\n')							print('[#] ' + data)				except:					pass		if 'b=131' in r13:			req = requests.get('https://search.yahoo.com/search?q=ip:'+url+'&first=131&b=131', headers=headers)			r14 = req.text			soup = BeautifulSoup(r14, 'html.parser')			re = soup.find('ol', {'class': 'mb-15 reg searchCenterMiddle'})			l = str(re.findAll('span', {'class': 'fz-ms fw-m fc-12th wr-bw lh-17'}))			for item in l:				try:					a = (l.replace('<span class="fz-ms fw-m fc-12th wr-bw lh-17">', 'http://'))					b = (a.replace('</span>', '/'))					c = (b.replace('<b>', ''))					f = (c.replace('</b>', ''))					g = (f.replace(' ', ''))					h = (g.replace('[', ''))					i = (h.replace(']', ''))					words = str(i).split(",")					for i in words:						open('links.txt', 'a+').close()						data = urlparse(i).hostname						if data not in open('links.txt', 'r').read():							open('links.txt', 'a+').write(data + '\n')							print('[#] ' + data)				except:					pass		if 'b=141' in r14:			req = requests.get('https://search.yahoo.com/search?q=ip:'+url+'&first=141&b=141', headers=headers)			r15 = req.text			soup = BeautifulSoup(r15, 'html.parser')			re = soup.find('ol', {'class': 'mb-15 reg searchCenterMiddle'})			l = str(re.findAll('span', {'class': 'fz-ms fw-m fc-12th wr-bw lh-17'}))			for item in l:				try:					a = (l.replace('<span class="fz-ms fw-m fc-12th wr-bw lh-17">', 'http://'))					b = (a.replace('</span>', '/'))					c = (b.replace('<b>', ''))					f = (c.replace('</b>', ''))					g = (f.replace(' ', ''))					h = (g.replace('[', ''))					i = (h.replace(']', ''))					words = str(i).split(",")					for i in words:						open('links.txt', 'a+').close()						data = urlparse(i).hostname						if data not in open('links.txt', 'r').read():							open('links.txt', 'a+').write(data + '\n')							print('[#] ' + data)				except:					pass		if 'b=151' in r15:			req = requests.get('https://search.yahoo.com/search?q=ip:'+url+'&first=151&b=151', headers=headers)			r16 = req.text			soup = BeautifulSoup(r16, 'html.parser')			re = soup.find('ol', {'class': 'mb-15 reg searchCenterMiddle'})			l = str(re.findAll('span', {'class': 'fz-ms fw-m fc-12th wr-bw lh-17'}))			for item in l:				try:					a = (l.replace('<span class="fz-ms fw-m fc-12th wr-bw lh-17">', 'http://'))					b = (a.replace('</span>', '/'))					c = (b.replace('<b>', ''))					f = (c.replace('</b>', ''))					g = (f.replace(' ', ''))					h = (g.replace('[', ''))					i = (h.replace(']', ''))					words = str(i).split(",")					for i in words:						open('links.txt', 'a+').close()						data = urlparse(i).hostname						if data not in open('links.txt', 'r').read():							open('links.txt', 'a+').write(data + '\n')							print('[#] ' + data)				except:					pass		if 'b=161' in r16:			req = requests.get('https://search.yahoo.com/search?q=ip:'+url+'&first=161&b=161', headers=headers)			r17 = req.text			soup = BeautifulSoup(r17, 'html.parser')			re = soup.find('ol', {'class': 'mb-15 reg searchCenterMiddle'})			l = str(re.findAll('span', {'class': 'fz-ms fw-m fc-12th wr-bw lh-17'}))			for item in l:				try:					a = (l.replace('<span class="fz-ms fw-m fc-12th wr-bw lh-17">', 'http://'))					b = (a.replace('</span>', '/'))					c = (b.replace('<b>', ''))					f = (c.replace('</b>', ''))					g = (f.replace(' ', ''))					h = (g.replace('[', ''))					i = (h.replace(']', ''))					words = str(i).split(",")					for i in words:						open('links.txt', 'a+').close()						data = urlparse(i).hostname						if data not in open('links.txt', 'r').read():							open('links.txt', 'a+').write(data + '\n')							print('[#] ' + data)				except:					pass		if 'b=171' in r17:			req = requests.get('https://search.yahoo.com/search?q=ip:'+url+'&first=171&b=171', headers=headers)			r18 = req.text			soup = BeautifulSoup(r18, 'html.parser')			re = soup.find('ol', {'class': 'mb-15 reg searchCenterMiddle'})			l = str(re.findAll('span', {'class': 'fz-ms fw-m fc-12th wr-bw lh-17'}))			for item in l:				try:					a = (l.replace('<span class="fz-ms fw-m fc-12th wr-bw lh-17">', 'http://'))					b = (a.replace('</span>', '/'))					c = (b.replace('<b>', ''))					f = (c.replace('</b>', ''))					g = (f.replace(' ', ''))					h = (g.replace('[', ''))					i = (h.replace(']', ''))					words = str(i).split(",")					for i in words:						open('links.txt', 'a+').close()						data = urlparse(i).hostname						if data not in open('links.txt', 'r').read():							open('links.txt', 'a+').write(data + '\n')							print('[#] ' + data)				except:					pass		if 'b=181' in r18:			req = requests.get('https://search.yahoo.com/search?q=ip:'+url+'&first=181&b=181', headers=headers)			r19 = req.text			soup = BeautifulSoup(r19, 'html.parser')			re = soup.find('ol', {'class': 'mb-15 reg searchCenterMiddle'})			l = str(re.findAll('span', {'class': 'fz-ms fw-m fc-12th wr-bw lh-17'}))			for item in l:				try:					a = (l.replace('<span class="fz-ms fw-m fc-12th wr-bw lh-17">', 'http://'))					b = (a.replace('</span>', '/'))					c = (b.replace('<b>', ''))					f = (c.replace('</b>', ''))					g = (f.replace(' ', ''))					h = (g.replace('[', ''))					i = (h.replace(']', ''))					words = str(i).split(",")					for i in words:						open('links.txt', 'a+').close()						data = urlparse(i).hostname						if data not in open('links.txt', 'r').read():							open('links.txt', 'a+').write(data + '\n')							print('[#] ' + data)				except:					pass		if 'b=191' in r19:			req = requests.get('https://search.yahoo.com/search?q=ip:'+url+'&first=191&b=191', headers=headers)			r20 = req.text			soup = BeautifulSoup(r20, 'html.parser')			re = soup.find('ol', {'class': 'mb-15 reg searchCenterMiddle'})			l = str(re.findAll('span', {'class': 'fz-ms fw-m fc-12th wr-bw lh-17'}))			for item in l:				try:					a = (l.replace('<span class="fz-ms fw-m fc-12th wr-bw lh-17">', 'http://'))					b = (a.replace('</span>', '/'))					c = (b.replace('<b>', ''))					f = (c.replace('</b>', ''))					g = (f.replace(' ', ''))					h = (g.replace('[', ''))					i = (h.replace(']', ''))					words = str(i).split(",")					for i in words:						open('links.txt', 'a+').close()						data = urlparse(i).hostname						if data not in open('links.txt', 'r').read():							open('links.txt', 'a+').write(data + '\n')							print('[#] ' + data)				except:					pass	except:		passif __name__ == '__main__':	p = ThreadPool(20)	result = p.map(BingBot, sites)	p.close()	p.join()